{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187cbf1d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a68103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import copy\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdbadc",
   "metadata": {},
   "source": [
    "## Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55608c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_classes = 37 #Number of classes in dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 224\n",
    "batch_size = 128\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e40f1",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    params_to_update = list(model.named_parameters())\n",
    "    \n",
    "    if feature_extracting:\n",
    "        for name, param in params_to_update:\n",
    "            param.requires_grad = False      \n",
    "    else:\n",
    "        for module, param in zip(model.modules(), model.parameters()):\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "            \n",
    "\n",
    "def grid_search_params(model, feature_extracting, perc):\n",
    "    params_to_update = list(model.named_parameters())\n",
    "    \n",
    "    i = 0\n",
    "   \n",
    "    for name, param in reversed(params_to_update):\n",
    "        if (i/len(params_to_update)) < perc:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "        i+=1\n",
    "    \n",
    "    for module, param in zip(model.modules(), model.parameters()):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            param.requires_grad = False\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extracting): #Initialize Resnet\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "\n",
    "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) \n",
    "        \n",
    "        set_parameter_requires_grad(model, feature_extracting)\n",
    "        \n",
    "        num_ftrs = model.fc.in_features\n",
    "        head = nn.Sequential(nn.BatchNorm1d(num_ftrs),\n",
    "                             nn.ReLU(),\n",
    "     ##                        nn.Dropout(0.25),#\n",
    "      #          nn.Linear(num_ftrs, 512),\n",
    "      #          nn.BatchNorm1d(512),\n",
    "      #          nn.ReLU(),\n",
    "     ##           nn.Dropout(0.25),#\n",
    "      #          nn.Linear(512, 256),\n",
    "      #          nn.BatchNorm1d(256),\n",
    "       #         nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "               nn.Linear(num_ftrs, num_classes))        \n",
    "        #model.fc = nn.Linear(num_ftrs,num_classes) # Update last layer to binary classification (Dog/cat)\n",
    "        model.fc = head#n.Linear(num_ftrs,num_classes) # Update last layer to binary classification (Dog/cat)\n",
    "        input_size = 224 #\"Finally, notice that inception_v3 requires the input size to be (299,299), whereas all of the other models expect (224,224).\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "    return model, input_size\n",
    "\n",
    "\n",
    "\n",
    "def readData(target_type):# = 'breeds'):\n",
    "    breed_to_species = {0:0, 1:1, 2:1, 3:1, 4:1, 5:0, 6:0, 7:0, 8:1, 9:0, 10:1,\n",
    "                    11:0, 12:1, 13:1, 14:1, 15:1, 16:1, #17:1, 18:1, 19:1, 20:0,\n",
    "                    21:1, 22:1, 23:0, 24:1, 25:1, 26:0, 27:0, 28:1, 29:1, 30:1,\n",
    "                    31:1, 32:0, 33:0, 34:1, 35:1, 36:1\n",
    "                   }\n",
    "    \n",
    "    transform_train = T.Compose([\n",
    "                           #T.Resize(256),\n",
    "                            T.Resize(image_size),\n",
    "\n",
    "                           T.CenterCrop(image_size),\n",
    "                           T.RandomHorizontalFlip(p=0.2),\n",
    "                           T.RandomVerticalFlip(p=0.2),\n",
    "                           ##T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                           #T.RandomRotation(degrees=90),\n",
    "                           T.ToTensor(),\n",
    "                           T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                           #T.RandomResizedCrop(size=image_size)\n",
    "                          ])\n",
    "    \n",
    "\n",
    "    \n",
    "    transform_test = T.Compose([T.Resize(image_size),\n",
    "        #T.Resize(256),\n",
    "                            T.CenterCrop(image_size),\n",
    "                           T.ToTensor(),\n",
    "                           T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                          ])\n",
    "    if target_type == 'species':\n",
    "        target_function = (lambda x: breed_to_species.get(x))\n",
    "    elif target_type == 'breeds':\n",
    "        target_function = None\n",
    "    else:\n",
    "        raise Exception(\"Wrong target type\")\n",
    "        \n",
    "    trainval = OxfordIIITPet(path,\n",
    "                             target_types= \"category\",\n",
    "                             transform=transform_train,\n",
    "                             split = 'trainval',\n",
    "                             target_transform = target_function\n",
    "                            )\n",
    "    test = OxfordIIITPet(path,\n",
    "                         target_types= \"category\",\n",
    "                         transform=transform_test,\n",
    "                         split = 'test',\n",
    "                         target_transform = target_function\n",
    "                        )\n",
    "                         \n",
    "    #train, val = splitData(trainval, 0.95, 0.05)\n",
    "    return trainval, test #train, val, test\n",
    "    \n",
    "def splitData(dataset,nrTrain,nrVal):\n",
    "    len1 = int(nrTrain*len(dataset))\n",
    "    len2 = int(len(dataset) - len1)\n",
    "    train, val = torch.utils.data.random_split(dataset, [len1,len2])\n",
    "    return train, val\n",
    "\n",
    "def show_example(img, label):\n",
    "    print('Label: ', train_dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler = None, num_epochs=25):\n",
    "    since = time.time()\n",
    "    loss_history = []\n",
    "    val_acc_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += float(loss.item() * inputs.size(0))\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            #if phase == 'train' and scheduler == None:\n",
    "                #scheduler.step()\n",
    "            \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            loss_history.append(epoch_loss)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            acc_history.append(epoch_acc)\n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history,loss_history,acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, dataloader, criterion):\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    print('Acc: {:4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6bd68",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a19b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val    = readData(\n",
    "                            #target_type = 'species'\n",
    "                            target_type = 'breeds'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, _ = splitData(train, 0.20, 0.8)\n",
    "#val, _ = splitData(val, 0.20,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d634ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,input_size = initialize_model(model_name = \"resnet\",\n",
    "                                    num_classes = 37,\n",
    "                                    feature_extracting = True)\n",
    "\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0),\n",
    "                                         \n",
    "               'val': torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=1e-3, \n",
    "                       #weight_decay=1e-5,\n",
    "                       #betas=(0.9, 0.999),\n",
    "                      )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=1e-2,\n",
    "                                                steps_per_epoch=len(dataloaders['train']), epochs=epochs)\n",
    "\n",
    "\n",
    "model, val_acc_history, loss_history, acc_history = train_model(model = model,\n",
    "                                          num_epochs = epochs,\n",
    "                                          dataloaders = dataloaders,\n",
    "                                          criterion = criterion,\n",
    "                                          optimizer = optimizer,\n",
    "                                          scheduler = None#scheduler\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(model, test_dataloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yvals = list(range(1,epochs+1))\n",
    "newloss = []\n",
    "newacc = []\n",
    "for i in range(len(loss_history)):\n",
    "    if i % 2 == 0:\n",
    "        newloss.append(loss_history[i])\n",
    "    else:\n",
    "        newacc.append(acc_history[i].cpu().item())\n",
    "\n",
    "plot1 = plt.figure(figsize=(10,5))\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(yvals,newloss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plot2 = plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(yvals,newacc)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "print(newacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac82461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"layer_names = []\n",
    "for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "\n",
    "layer_names.reverse()\n",
    "\n",
    "# learning rate\n",
    "lr      = 1e-3\n",
    "lr_mult = 0.9\n",
    "\n",
    "# placeholder\n",
    "parameters = []\n",
    "\n",
    "# store params & learning rates\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    # display info\n",
    "    print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "    \n",
    "    # append layer parameters\n",
    "    parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                    'lr':     lr}]\n",
    "    \n",
    "    # update learning rate\n",
    "    lr *= lr_mult\n",
    "\n",
    "optimizer = optim.Adam(parameters)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecc746",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "111f9612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/6\n",
      "----------\n",
      "train Loss: 0.4505 Acc: 0.9196\n",
      "val Loss: 0.4777 Acc: 0.8844\n",
      "Epoch 1/6\n",
      "----------\n",
      "train Loss: 0.2862 Acc: 0.9541\n",
      "val Loss: 0.4266 Acc: 0.8863\n",
      "Epoch 2/6\n",
      "----------\n",
      "train Loss: 0.1342 Acc: 0.9883\n",
      "val Loss: 0.3823 Acc: 0.8962\n",
      "Epoch 3/6\n",
      "----------\n",
      "train Loss: 0.0737 Acc: 0.9997\n",
      "val Loss: 0.3594 Acc: 0.9019\n",
      "Epoch 4/6\n",
      "----------\n",
      "train Loss: 0.0513 Acc: 1.0000\n",
      "val Loss: 0.3501 Acc: 0.9019\n",
      "Epoch 5/6\n",
      "----------\n",
      "train Loss: 0.0431 Acc: 1.0000\n",
      "val Loss: 0.3467 Acc: 0.9030\n",
      "Epoch 6/6\n",
      "----------\n",
      "train Loss: 0.0399 Acc: 1.0000\n",
      "val Loss: 0.3443 Acc: 0.9043\n",
      "Training complete in 5m 26s\n",
      "Best val Acc: 0.904334\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tensor'))\n",
    "set_parameter_requires_grad(model, False)\n",
    "\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=1e-7, \n",
    "                       #weight_decay=1e-5,\n",
    "                       #betas=(0.9, 0.999),\n",
    "                      )\n",
    "\n",
    "finetune_epochs = 7\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=1e-4,\n",
    "                                                steps_per_epoch=len(dataloaders['train']),\n",
    "                                                epochs=finetune_epochs)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, 0.5)\n",
    "\n",
    "best_model, val_acc_history, loss_history, acc_history = train_model(model = model,\n",
    "                                          num_epochs = finetune_epochs,\n",
    "                                          dataloaders = dataloaders,\n",
    "                                          criterion = criterion,\n",
    "                                          optimizer = optimizer,\n",
    "                                          scheduler = scheduler\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6673d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score(best_model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e77d8",
   "metadata": {},
   "source": [
    "## Grid search layer params config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('tensor'))\n",
    "set_parameter_requires_grad(model, False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                      lr=1e-7, \n",
    "                      weight_decay=1e-5,\n",
    "                      betas=(0.9, 0.999),\n",
    "                      )\n",
    "\n",
    "finetune_epochs = 7\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=1e-5,\n",
    "                                                steps_per_epoch=len(dataloaders['train']),\n",
    "                                                epochs=finetune_epochs)\n",
    "\n",
    "best_model, val_acc_history, loss_history, acc_history = train_model(model = model,\n",
    "                                          num_epochs = finetune_epochs,\n",
    "                                          dataloaders = dataloaders,\n",
    "                                          criterion = criterion,\n",
    "                                          optimizer = optimizer,\n",
    "                                          scheduler = scheduler\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569f3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd82e6248dbff03038d799f8cb0a4b7e2d88f22b02718fef3b6ea553d7792884"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
