{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187cbf1d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a68103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import copy\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdbadc",
   "metadata": {},
   "source": [
    "## Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55608c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da150x_47/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_classes = 37 #Number of classes in dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 224\n",
    "batch_size = 128\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "path = \"Deep_Learning_Project-1/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e40f1",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0571b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    params_to_update = list(model.named_parameters())\n",
    "    \n",
    "    if feature_extracting:\n",
    "        for name, param in params_to_update:\n",
    "            param.requires_grad = False      \n",
    "    else:\n",
    "        for module, param in zip(model.modules(), model.parameters()):\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extracting): #Initialize Resnet\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "\n",
    "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) \n",
    "        \n",
    "        set_parameter_requires_grad(model, feature_extracting)\n",
    "        \n",
    "        num_ftrs = model.fc.in_features\n",
    "        head = nn.Sequential(nn.BatchNorm1d(num_ftrs),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(0.25),\n",
    "                nn.Linear(num_ftrs, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(256, num_classes))\n",
    "        \n",
    "        model.fc = nn.Linear(num_ftrs,num_classes) # Update last layer to binary classification (Dog/cat)\n",
    "        input_size = 224 #\"Finally, notice that inception_v3 requires the input size to be (299,299), whereas all of the other models expect (224,224).\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "    return model, input_size\n",
    "\n",
    "\n",
    "\n",
    "def readData(target_type = 'breeds'):\n",
    "    breed_to_species = {0:0, 1:1, 2:1, 3:1, 4:1, 5:0, 6:0, 7:0, 8:1, 9:0, 10:1,\n",
    "                    11:0, 12:1, 13:1, 14:1, 15:1, 16:1, 17:1, 18:1, 19:1, 20:0,\n",
    "                    21:1, 22:1, 23:0, 24:1, 25:1, 26:0, 27:0, 28:1, 29:1, 30:1,\n",
    "                    31:1, 32:0, 33:0, 34:1, 35:1, 36:1\n",
    "                   }\n",
    "    \n",
    "    transform = T.Compose([T.Resize(image_size),\n",
    "                           T.CenterCrop(image_size),\n",
    "                           T.ToTensor(),\n",
    "                           T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                           T.RandomHorizontalFlip(p=0.5),\n",
    "                           #T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                           #T.RandomHorizontalFlip(),\n",
    "                           #T.RandomRotation(degrees=60),\n",
    "                           #T.RandomResizedCrop(size=image_size)\n",
    "                          ])\n",
    "    \n",
    "    if target_type == 'species':\n",
    "        target_function = (lambda x: breed_to_species.get(x))\n",
    "    elif target_type == 'breeds':\n",
    "        target_function = None\n",
    "    else:\n",
    "        raise Exception(\"Wrong target type\")\n",
    "        \n",
    "    trainval = OxfordIIITPet(path,\n",
    "                             target_types= \"category\",\n",
    "                             transform=transform,\n",
    "                             split = 'trainval',\n",
    "                             target_transform = target_function\n",
    "                            )\n",
    "    test = OxfordIIITPet(path,\n",
    "                         target_types= \"category\",\n",
    "                         transform=transform,\n",
    "                         split = 'test',\n",
    "                         target_transform = target_function\n",
    "                        )\n",
    "                         \n",
    "    #train, val = splitData(trainval, 0.95, 0.05)\n",
    "    return trainval, test #train, val, test\n",
    "    \n",
    "def splitData(dataset,nrTrain,nrVal):\n",
    "    len1 = int(nrTrain*len(dataset))\n",
    "    len2 = int(len(dataset) - len1)\n",
    "    train, val = torch.utils.data.random_split(dataset, [len1,len2])\n",
    "    return train, val\n",
    "\n",
    "def show_example(img, label):\n",
    "    print('Label: ', train_dataset.classes[label], \"(\"+str(label)+\")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2c0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                pass#scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c280de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, dataloader):\n",
    "    running_corrects = 0\n",
    "    model.eval()\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    acc = running_corrects.double() / len(dataloader.dataset)\n",
    "    print('Acc: {:4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6bd68",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a19b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val    = readData(\n",
    "                            #target_type = 'species'\n",
    "                            target_type = 'breeds'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a1d8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, _ = splitData(train, 0.20, 0.8)\n",
    "#val, _ = splitData(val, 0.20,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d634ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/da150x_47/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model,input_size = initialize_model(model_name = \"resnet\",\n",
    "                                    num_classes = 37,\n",
    "                                    feature_extracting = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e15b1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 3.5222 Acc: 0.0712\n",
      "val Loss: 2.9231 Acc: 0.3129\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 2.1224 Acc: 0.6011\n",
      "val Loss: 1.4251 Acc: 0.7779\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 1.1288 Acc: 0.8370\n",
      "val Loss: 0.9773 Acc: 0.8427\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.8451 Acc: 0.8780\n",
      "val Loss: 0.8597 Acc: 0.8555\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.7706 Acc: 0.9000\n",
      "val Loss: 0.8421 Acc: 0.8545\n",
      "Training complete in 13m 33s\n",
      "Best val Acc: 0.855546\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {label: torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=4)\n",
    "          for label, data in zip(['train', 'val'],[train, val])}\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=1e-7, \n",
    "                       weight_decay=1e-5,\n",
    "                       betas=(0.9, 0.999),\n",
    "                      )\n",
    "\n",
    "\n",
    "\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=3,gamma=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=1e-3,\n",
    "                                                steps_per_epoch=len(dataloaders['train']), epochs=epochs)\n",
    "\n",
    "\n",
    "best_model, val_acc_history = train_model(model = model,\n",
    "                                          num_epochs = epochs,\n",
    "                                          dataloaders = dataloaders,\n",
    "                                          criterion = criterion,\n",
    "                                          optimizer = optimizer,\n",
    "                                          scheduler = scheduler\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0a760b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5b00085bcc45309b0fa89701b748c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.856909\n"
     ]
    }
   ],
   "source": [
    "score(best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa81c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "111f9612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/7\n",
      "----------\n",
      "train Loss: 0.7091 Acc: 0.9035\n",
      "val Loss: 0.6186 Acc: 0.8798\n",
      "Epoch 1/7\n",
      "----------\n",
      "train Loss: 0.4484 Acc: 0.9299\n",
      "val Loss: 0.4527 Acc: 0.8972\n",
      "Epoch 2/7\n",
      "----------\n",
      "train Loss: 0.2436 Acc: 0.9652\n",
      "val Loss: 0.3963 Acc: 0.9011\n",
      "Epoch 3/7\n",
      "----------\n",
      "train Loss: 0.1375 Acc: 0.9891\n",
      "val Loss: 0.3678 Acc: 0.9024\n",
      "Epoch 4/7\n",
      "----------\n",
      "train Loss: 0.0889 Acc: 0.9932\n",
      "val Loss: 0.3460 Acc: 0.9062\n",
      "Epoch 5/7\n",
      "----------\n",
      "train Loss: 0.0663 Acc: 0.9978\n",
      "val Loss: 0.3386 Acc: 0.9111\n",
      "Epoch 6/7\n",
      "----------\n",
      "train Loss: 0.0550 Acc: 0.9995\n",
      "val Loss: 0.3365 Acc: 0.9081\n",
      "Epoch 7/7\n",
      "----------\n",
      "train Loss: 0.0517 Acc: 1.0000\n",
      "val Loss: 0.3341 Acc: 0.9087\n",
      "Training complete in 34m 50s\n",
      "Best val Acc: 0.911147\n"
     ]
    }
   ],
   "source": [
    "set_parameter_requires_grad(best_model, False)\n",
    "\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=1e-6, \n",
    "                       weight_decay=1e-5,\n",
    "                       betas=(0.9, 0.999),\n",
    "                      )\n",
    "\n",
    "finetune_epochs = 8\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=1e-4,\n",
    "                                                steps_per_epoch=len(dataloaders['train']),\n",
    "                                                epochs=finetune_epochs)\n",
    "\n",
    "best_model, val_acc_history = train_model(model = best_model,\n",
    "                                          num_epochs = finetune_epochs,\n",
    "                                          dataloaders = dataloaders,\n",
    "                                          criterion = criterion,\n",
    "                                          optimizer = optimizer,\n",
    "                                          scheduler = scheduler\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6673d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e61f345d0a472c87c43c901d4ecf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.910330\n"
     ]
    }
   ],
   "source": [
    "score(best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547af34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bada68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
